{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as spr\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('/Users/michelle/Data Science/Melon Playlist Continuation/train.json', typ = 'frame')\n",
    "test = pd.read_json('/Users/michelle/Data Science/Melon Playlist Continuation/val.json', typ= 'frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['istrain'] = 1\n",
    "test['istrain'] = 0\n",
    "\n",
    "n_train = len(train)\n",
    "n_test = len(test)\n",
    "\n",
    "# train + test\n",
    "plylst = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "# playlist id\n",
    "plylst[\"nid\"] = range(n_train + n_test)\n",
    "\n",
    "# id <-> nid\n",
    "plylst_id_nid = dict(zip(plylst[\"id\"],plylst[\"nid\"]))\n",
    "plylst_nid_id = dict(zip(plylst[\"nid\"],plylst[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plylst_tag = plylst['tags']\n",
    "tag_counter = Counter([tg for tgs in plylst_tag for tg in tgs])\n",
    "tag_dict = {x: tag_counter[x] for x in tag_counter}\n",
    "\n",
    "tag_id_tid = dict()\n",
    "tag_tid_id = dict()\n",
    "for i, t in enumerate(tag_dict):\n",
    "  tag_id_tid[t] = i\n",
    "  tag_tid_id[i] = t\n",
    "\n",
    "n_tags = len(tag_dict)\n",
    "\n",
    "plylst_song = plylst['songs']\n",
    "song_counter = Counter([sg for sgs in plylst_song for sg in sgs])\n",
    "song_dict = {x: song_counter[x] for x in song_counter}\n",
    "\n",
    "song_id_sid = dict()\n",
    "song_sid_id = dict()\n",
    "for i, t in enumerate(song_dict):\n",
    "  song_id_sid[t] = i\n",
    "  song_sid_id[i] = t\n",
    "\n",
    "n_songs = len(song_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plylst['songs_id'] = plylst['songs'].map(lambda x: [song_id_sid.get(s) for s in x if song_id_sid.get(s) != None])\n",
    "plylst['tags_id'] = plylst['tags'].map(lambda x: [tag_id_tid.get(t) for t in x if tag_id_tid.get(t) != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plylst_use = plylst.loc[:,['istrain','nid','updt_date','songs_id','tags_id']]\n",
    "plylst_use.loc[:,'num_songs'] = plylst_use['songs_id'].map(len)\n",
    "plylst_use.loc[:,'num_tags'] = plylst_use['tags_id'].map(len)\n",
    "plylst_use = plylst_use.set_index('nid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = np.repeat(range(n_train+n_test), plylst_use['num_songs'])\n",
    "col = [song for songs in plylst_use['songs_id'] for song in songs]\n",
    "dat = np.repeat(1, plylst_use['num_songs'].sum())\n",
    "all_songs = spr.csr_matrix((dat, (row, col)), shape=(n_train+n_test, n_songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138086, 638336)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_songs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138086"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train) + len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_songs = all_songs[:len(train), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAE modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    # coo = coo.tolil()  ## tolil로 먼저 변경하기\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = convert_sparse_matrix_to_sparse_tensor(train_songs)\n",
    "# kernel dies: train_x = tf.sparse_tensor_to_dense(train_x)\n",
    "train_y = train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([115071, 638336])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x_vals, y_vals, batch_size = 32):\n",
    "        self.x_vals = x_vals\n",
    "        self.y_vals = y_vals\n",
    "        self.inds = list(range(x_vals.shape[0]))\n",
    "        shuffle(self.inds)\n",
    "        self.batch_size = batch_size\n",
    "    def __getitem__(self, item):\n",
    "        from_ind = self.batch_size * item\n",
    "        to_ind = self.batch_size * (item + 1)\n",
    "        return (self.x_vals[self.inds[from_ind:to_ind], :].todense(),\n",
    "                y_vals[self.inds[from_ind:to_ind]])\n",
    "    def on_epoch_end(self):\n",
    "        shuffle(self.inds)\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.x_vals.shape[0] / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## layer parameters\n",
    "noise_level = 1.0\n",
    "n_inputs = 638336\n",
    "n_hidden1 = 256  # encoder\n",
    "n_hidden2 = 128  # coding units\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "## train parameters\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.01\n",
    "n_epochs = 5\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dropout(dropout_rate, input_shape=(1, n_inputs)),\n",
    "    Dense(n_hidden1, activation='sigmoid', name='hidden1'),\n",
    "    Dense(n_hidden2, activation='sigmoid', name='hidden2'),\n",
    "    Dense(n_hidden3, activation='sigmoid', name='hidden3'),\n",
    "    Dense(n_outputs, name='outputs')\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 1, 638336)         0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 1, 1024)           653657088 \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 1, 512)            524800    \n",
      "_________________________________________________________________\n",
      "hidden3 (Dense)              (None, 1, 1024)           525312    \n",
      "_________________________________________________________________\n",
      "outputs (Dense)              (None, 1, 638336)         654294400 \n",
      "=================================================================\n",
      "Total params: 1,309,001,600\n",
      "Trainable params: 1,309,001,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(x_vals, y_vals):\n",
    "    inds = list(range(x_vals.shape[0]))\n",
    "    shuffle(inds)\n",
    "    for ind in inds:\n",
    "        yield (x_vals[ind, :].todense(), y_vals[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int64, but the yielded element was   (0, 1840)\t1\n  (0, 2183)\t1\n  (0, 2204)\t1\n  (0, 2205)\t1\n  (0, 4659)\t1\n  (0, 18050)\t1\n  (0, 21613)\t1\n  (0, 37266)\t1.\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'csr_matrix'\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 801, in generator_py_func\n    ret, dtype=dtype.as_numpy_dtype))\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 203, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nValueError: setting an array element with a sequence.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 806, in generator_py_func\n    \"element was %s.\" % (dtype.name, ret)), sys.exc_info()[2])\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/six.py\", line 702, in reraise\n    raise value.with_traceback(tb)\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 801, in generator_py_func\n    ret, dtype=dtype.as_numpy_dtype))\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 203, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int64, but the yielded element was   (0, 1840)\t1\n  (0, 2183)\t1\n  (0, 2204)\t1\n  (0, 2205)\t1\n  (0, 4659)\t1\n  (0, 18050)\t1\n  (0, 21613)\t1\n  (0, 37266)\t1.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_856]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c3607d006fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_songs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_songs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# We pass some validation for monitoring validation loss and metrics at the end of each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# validation_data=(x_val, y_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int64, but the yielded element was   (0, 1840)\t1\n  (0, 2183)\t1\n  (0, 2204)\t1\n  (0, 2205)\t1\n  (0, 4659)\t1\n  (0, 18050)\t1\n  (0, 21613)\t1\n  (0, 37266)\t1.\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'csr_matrix'\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 801, in generator_py_func\n    ret, dtype=dtype.as_numpy_dtype))\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 203, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nValueError: setting an array element with a sequence.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\n    ret = func(*args)\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 806, in generator_py_func\n    \"element was %s.\" % (dtype.name, ret)), sys.exc_info()[2])\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/six.py\", line 702, in reraise\n    raise value.with_traceback(tb)\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 801, in generator_py_func\n    ret, dtype=dtype.as_numpy_dtype))\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 203, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/Users/michelle/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\n    return array(a, dtype, copy=False, order=order)\n\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int64, but the yielded element was   (0, 1840)\t1\n  (0, 2183)\t1\n  (0, 2204)\t1\n  (0, 2205)\t1\n  (0, 4659)\t1\n  (0, 18050)\t1\n  (0, 21613)\t1\n  (0, 37266)\t1.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_856]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "autoencoder = model.fit(\n",
    "    data_generator(train_songs, train_songs),\n",
    "    batch_size = batch_size,\n",
    "    epochs = n_epochs,\n",
    "    # We pass some validation for monitoring validation loss and metrics at the end of each epoch\n",
    "    # validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = train_songs.toarray()\n",
    "# kernel dies: ex = tf.convert_to_tensor(a, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "tf.random.set_seed(11)\n",
    "batch_size = 1\n",
    "max_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "momentum = 8e-1\n",
    "hidden_dim = 1024\n",
    "original_dim = 638336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 부분을 Denoising Encoder로 바꾸기 위해 dropout을 추가한다\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.dropout_layer = tf.keras.layers.Dropout(rate=0.3)\n",
    "        self.hidden_layer = tf.keras.layers.Dense(units=hidden_dim, activation=tf.nn.relu)\n",
    "    \n",
    "    def call(self, input_features):\n",
    "        #dropped = self.dropout_layer(input_features)\n",
    "        #activation = self.hidden_layer(dropped)\n",
    "        activation = self.hidden_layer(input_features)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, original_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_layer = tf.keras.layers.Dense(units=original_dim, activation=tf.nn.relu)\n",
    "  \n",
    "    def call(self, encoded):\n",
    "        activation = self.output_layer(encoded)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim, original_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.loss = []\n",
    "        self.encoder = Encoder(hidden_dim=hidden_dim)\n",
    "        self.decoder = Decoder(hidden_dim=hidden_dim, original_dim=original_dim)\n",
    "\n",
    "    def call(self, input_features):\n",
    "        encoded = self.encoder(input_features)\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "loss = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size)\n",
    "# noisy data를 따로 만들지 않고 dropout을 시켜도록 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**막힌 부분: input을 어떻게 만들어야 하는가? 각 row를 train 하나하나로 넣으려면?  \n",
    "그리고 그렇게 하면 각 input이 (1, 638336) sparse vector인데, train 시간이 너무 오래 걸리지 않을까?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAE codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**질문: class 잘 쓸 줄 모름...__init___은 무슨 함수이고, 여기서 self와 conf은 무엇인가?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE_tied():\n",
    "    def __init__(self, conf):\n",
    "        self.save_dir = conf.save\n",
    "\n",
    "        self.n_batch = conf.batch\n",
    "        self.n_input = conf.n_input\n",
    "        self.n_hidden = conf.hidden\n",
    "        self.learning_rate = conf.lr\n",
    "        self.reg_lambda = conf.reg_lambda\n",
    "\n",
    "        self.x_positions = tf.placeholder(dtype=tf.int64,shape=[None,2])\n",
    "        self.x_ones = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "        self.y_positions = tf.placeholder(dtype=tf.int64,shape=[None,2])\n",
    "        self.y_ones = tf.placeholder(dtype=tf.float32)\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32, shape=[])\n",
    "        self.input_keep_prob = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x_sparse = tf.SparseTensor(indices=self.x_positions,\n",
    "                                       values=self.x_ones,dense_shape=[self.n_batch,self.n_input])\n",
    "            self.x = tf.sparse_tensor_to_dense(x_sparse, validate_indices=False)\n",
    "            y_sparse = tf.SparseTensor(indices=self.y_positions,\n",
    "                                       values=self.y_ones,dense_shape=[self.n_batch,self.n_input])\n",
    "            self.y = tf.sparse_tensor_to_dense(y_sparse, validate_indices=False)\n",
    "\n",
    "        x_dropout = tf.nn.dropout(self.x, keep_prob=self.input_keep_prob)  # dropout 찾아보기\n",
    "        self.reduce_sum = tf.reduce_sum(x_dropout, 1, keepdims=True)\n",
    "        self.x_dropout = tf.divide(x_dropout, self.reduce_sum + 1e-10)\n",
    "\n",
    "        self.y_pred = None\n",
    "        self.cost = None\n",
    "        self.optimizer = None\n",
    "        self.init_op = None\n",
    "\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self.d_params = []\n",
    "\n",
    "    def init_weight(self):\n",
    "        self.weights['encoder_h'] = tf.get_variable(\"encoder_h\", shape=[self.n_input, self.n_hidden],\n",
    "                                                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.biases['encoder_b'] = tf.get_variable(name=\"encoder_b\", shape=[self.n_hidden],\n",
    "                                                   initializer=tf.zeros_initializer())\n",
    "        self.biases['decoder_b'] = tf.get_variable(name=\"decoder_b\", shape=[self.n_input],\n",
    "                                                   initializer=tf.zeros_initializer())\n",
    "        self.d_params = [self.weights['encoder_h'], self.weights['encoder_h'],\n",
    "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
    "\n",
    "    # Building the encoder\n",
    "    def encoder(self, x):\n",
    "        # Encoder Hidden layer with sigmoid activation #1         \n",
    "        layer = tf.add(tf.matmul(x, self.weights['encoder_h']), self.biases['encoder_b'])\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "        layer = tf.nn.dropout(layer, self.keep_prob)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    # Building the decoder\n",
    "    def decoder(self, x):\n",
    "        # Decoder Hidden layer with sigmoid activation #1\n",
    "        layer = tf.nn.sigmoid(tf.add(tf.matmul(x, tf.transpose(self.weights['encoder_h'])),\n",
    "                                   self.biases['decoder_b']))\n",
    "        return layer\n",
    "\n",
    "    def l2_loss(self):\n",
    "        l2 = tf.nn.l2_loss(self.weights['encoder_h']) + tf.nn.l2_loss(self.biases['decoder_b']) + \\\n",
    "             tf.nn.l2_loss(self.biases['encoder_b'])\n",
    "        return l2\n",
    "\n",
    "    def fit(self):\n",
    "        # Construct model\n",
    "        with tf.device(\"/cpu:0\"):  #CPU\n",
    "            self.init_weight()\n",
    "\n",
    "        encoder_op = self.encoder(self.x_dropout)\n",
    "        with tf.device(\"/gpu:1\"):  #GPU1\n",
    "            self.y_pred = self.decoder(encoder_op)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):  #CPU\n",
    "            l2 = self.l2_loss()\n",
    "            \n",
    "        # Define loss and optimizer, minimize the squared error\n",
    "        with tf.device(\"/gpu:1\"): ##SHOULD BE GPU1\n",
    "            L = -tf.reduce_sum(self.y*tf.log(self.y_pred+1e-10) + \n",
    "                               0.55*(1 - self.y)* tf.log(1 - self.y_pred+1e-10),axis = 1)\n",
    "            self.cost = tf.reduce_mean(L) + self.reg_lambda * l2\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Initialize the variables (i.e. assign their default value)\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    def save_model(self, sess):\n",
    "        param = sess.run(self.d_params)\n",
    "        output = open(self.save_dir, 'wb')\n",
    "        pickle.dump(param, output)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE(DAE_tied):\n",
    "    def __init__(self, conf):\n",
    "        DAE_tied.__init__(self, conf)\n",
    "        self.initval_dir = conf.initval\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.initval_dir == 'NULL':\n",
    "            self.weights['encoder_h'] = tf.get_variable(\"encoder_h\", shape=[self.n_input, self.n_hidden],\n",
    "                                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.weights['decoder_h'] = tf.get_variable(\"decoder_h\", shape=[self.n_input, self.n_hidden],\n",
    "                                                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.biases['encoder_b'] = tf.get_variable(name=\"encoder_b\", shape=[self.n_hidden],\n",
    "                                                       initializer=tf.zeros_initializer())\n",
    "            self.biases['decoder_b'] = tf.get_variable(name=\"decoder_b\", shape=[self.n_input],\n",
    "                                                       initializer=tf.zeros_initializer())\n",
    "        else:\n",
    "            with open(self.initval_dir, 'rb') as f:\n",
    "                emb = pickle.load(f)\n",
    "            self.weights['encoder_h'] = tf.get_variable(\"encoder_h\", initializer=tf.constant(emb[0]))\n",
    "            self.weights['decoder_h'] = tf.get_variable(\"decoder_h\", initializer=tf.constant(emb[1]))\n",
    "            self.biases['encoder_b'] = tf.get_variable(name=\"encoder_b\", initializer=tf.constant(emb[2]))\n",
    "            self.biases['decoder_b'] = tf.get_variable(name=\"decoder_b\", initializer=tf.constant(emb[3]))\n",
    "\n",
    "        self.d_params = [self.weights['encoder_h'], self.weights['decoder_h'],\n",
    "                         self.biases['encoder_b'], self.biases['decoder_b']]\n",
    "\n",
    "    # Building the decoder\n",
    "    def decoder(self, x):\n",
    "        # Decoder Hidden layer with sigmoid activation #1\n",
    "        layer = tf.nn.sigmoid(tf.add(tf.matmul(x, tf.transpose(self.weights['decoder_h'])),\n",
    "                                     self.biases['decoder_b']))\n",
    "        return layer\n",
    "\n",
    "    def l2_loss(self):\n",
    "        l2 = tf.nn.l2_loss(self.weights['encoder_h']) + tf.nn.l2_loss(self.biases['decoder_b']) + \\\n",
    "             tf.nn.l2_loss(self.biases['encoder_b']) + tf.nn.l2_loss(self.weights['decoder_h'])\n",
    "        return l2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
